{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00505bd6",
   "metadata": {},
   "source": [
    "# Obtaining, parsing and structuring static HTML websites\n",
    "\n",
    "In this notebook we will learn how to scrape basic static, i.e. non-interactive HTML-based websites. We will\n",
    "- obtain the HTML raw content using the `requests` module\n",
    "- convert the raw HTML into a format that is easier to search, or parse, using the `BeautifulSoup` module\n",
    "- learn how to identify the elements of interest in the raw HTML using the browser's inspect functionality and the CSS SelectorGadget\n",
    "- construct a table, or dataframe, with the popular table calculation module `pandas` and store the output locally in a standard spreadsheet format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6a337",
   "metadata": {},
   "source": [
    "1. Open the Anaconda Prompt and install the module `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f0033dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "245f6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'https://www.uni-potsdam.de/de/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7ca65",
   "metadata": {},
   "source": [
    "2. What data type is the object `seed`? How can you check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94dfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6577ae4e",
   "metadata": {},
   "source": [
    "3. Is this domain an admissible path? Hint: Check the `robots.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39c15aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba7a26",
   "metadata": {},
   "source": [
    "4. Was the request successful? How can you check the status? Hint: Check the available methods by using Jupyter's auto-complete functionality, i.e. type a dot at the end of the object you're investigating followed by <kbd>Tab</kbd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29a831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a2239b1",
   "metadata": {},
   "source": [
    "5. Which method could be most informative w.r.t. actual content? How many characters long is the raw HTML file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92d257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1bc53b3",
   "metadata": {},
   "source": [
    "6. Display the first 518 characters of the `html` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05bce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e32fe467",
   "metadata": {},
   "source": [
    "7. Display meta information on the origin of the HTTP request, e.g. date. Note that it is possible to specify the `user-agent` that the server receives and provides the response (website representation) such that it optimised, e.g. Desktop vs. mobile. If it's not specified, the request will be sent using default values (potentially) containing information about your operating system, screen resolution, keyboard language, IP address and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23744022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5ee423",
   "metadata": {},
   "source": [
    "The cell below saves the HTML object's text attribute in HTML format locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce1d0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Uni_Potsdam.html', 'w', encoding='utf-8') as f:\n",
    "    \n",
    "    f.write(html.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da94971",
   "metadata": {},
   "source": [
    "8. Install the module `BeautifulSoup` via `pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "909ff5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c2f8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c98e9",
   "metadata": {},
   "source": [
    "9. Parse the BeautifulSoup object `soup` for all Affiliate Links. Hint: In a HTML document all elements that lead to another domain are indicated by an `a` and follow the structure `<a href=\"...\", ... >text</a>`. Hint: Use `soup`'s method `find_all()` where the input argument is the elements' prefix. What object type is the output? Can you iterate over it? How many elements of an Affiliate Link type are contained in the HTML file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b517d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfc92243",
   "metadata": {},
   "source": [
    "10. Convert the BeautifulSoup object into a \"plain\" Python list object containing the elements' **text** attributes by iterating over it. Hint: Instantiate an empty `list` object, write a for-loop and `append` each element to the list object. You may also remove any unwanted whitespaces by using the `strip` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7aae91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_list = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    \n",
    "    empty_list.append(link.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a5535",
   "metadata": {},
   "source": [
    "#### Pro-Tipp\n",
    "Instead of explicitly writing a for-loop when disentangling specific objects from an aggregate object you can use Python's built-in `map` and `lambda` functions as a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "28dcb515",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = list(map(lambda x: x.text.strip(), soup.find_all('a')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6893c",
   "metadata": {},
   "source": [
    "11. Identify the element which text attribute's value is equal to \"alle Artikel\". Return the element's position (`index`) within the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24f2330e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list.index('alle Artikel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fd787",
   "metadata": {},
   "source": [
    "12. Obtain this element's value of the `href` attribute. It should be an URL pointing at the domain where the news at Universit√§t Potsdam are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "274d63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seed = soup.find_all('a')[224].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "25c697a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.uni-potsdam.de/nachrichten.html'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a9f9f",
   "metadata": {},
   "source": [
    "13. Write a function which takes a String-type object (e.g. an URL) as input and returns a readily parse-able `BeautifulSoup` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c45fd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def URL_to_BS(url):\n",
    "    \n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "76d13dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_soup = URL_to_BS(new_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02264a81",
   "metadata": {},
   "source": [
    "14. Open the `new_seed` URL in your browser and enable the CSS SelectorGadget. Highlight the box containing the first article. The other, similar boxes should be highlighted as well. Copy the identified CSS selector and parse through the `news_soup` object but this time over elements corresponding to the CSS selector you found (use `.select()` instead of `find_all()`). Store the subset of elements in a list. You can achieve all of this in one line of code. How many items does this list contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8f3b3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = list(map(lambda x: x, news_soup.select('.up-news-list-item')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "32c8a0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22698b48",
   "metadata": {},
   "source": [
    "15. Split the list's elements into their hyperlinks (`href`) and text attributes' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "949ac1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "title_list = []\n",
    "\n",
    "for link_num in range(len(news_list)):\n",
    "    \n",
    "    sub_link = news_list[link_num].findChild(\"a\")['href']\n",
    "    sub_title = news_list[link_num].findChild(\"a\")['title']\n",
    "    \n",
    "    if type(sub_link) is str and 'www' not in sub_link:\n",
    "        \n",
    "        link_list.append('https://www.uni-potsdam.de' + sub_link)\n",
    "        title_list.append(sub_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3680ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lot = list(zip(title_list, link_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "54b54191",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = dict(lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9f9d0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cc854f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Uni_Potsdam_dict.json', 'w', encoding='utf-8') as f:\n",
    "    \n",
    "    json.dump(D, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "caa44ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Uni_Potsdam_dict.json', 'r', encoding='utf-8') as f:\n",
    "    \n",
    "    D_read = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f4eb2b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D == D_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b1a92",
   "metadata": {},
   "source": [
    "## Pagination\n",
    "You have probably realised that the articles presented on the first news page are not the entire collection of the University of Potsdam. Your goal is to retrieve a complete collection of all articles that are available on the university's website and you can easily apply your new knowledge in a repetive manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ac12e",
   "metadata": {},
   "source": [
    "16. Figure out how many pages containing articles content there are in total. You can do it manually by e.g. inspecting the URL when you proceed through the collection in your browser or by checking it programmatically by writing a `while` loop that continues until some condition, such as a status returned from your request, is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406c42a",
   "metadata": {},
   "source": [
    "17. Once you have the maximum iteration amount, write a loop that passes through the pages and append each element to a list. Bonus: Take note of the page's number the element was found. Hint: You can use Python's `enumerate` method. Separate the hyperlinks and titles as above, combine them again to a tuple, convert it into a dictionary and save it as a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538df1bd",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "Actually, having a complete list of links is sufficient for the next task. We want to iterate over the entire articles collection and conduct a simple analysis that involves text analysis, image processing and publication record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef29f9e5",
   "metadata": {},
   "source": [
    "18. Read in the JSON file you stored in step 17 and iterate over each hyperlink. In each iteration, obtain the HTML, parse it and identify the elements of the publication date, the contact, the contact's email address, the image's hyperlink/reference and the main text body's length. Define an appropriate data type for each field and append it **as a dictionary** in each iteration to a list. Convert the final list into a `pandas` dataframe and save it as a `.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6c277",
   "metadata": {},
   "source": [
    "19. Convert the `publication_date` into a `pandas` `datetime` object and plot a time series of published articles on a daily basis. Bonus: Aggregate the time series into monthly frequency. In which month-year were most articles published?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Webscraping Workshop",
   "language": "python",
   "name": "webscraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
